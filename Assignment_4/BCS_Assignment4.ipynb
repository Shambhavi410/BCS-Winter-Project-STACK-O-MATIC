{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGXSAkJC9hFSwNSVzI0ss5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shambhavi410/BCS-Winter-Project-STACK-O-MATIC/blob/main/BCS_Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgvcAJR-zuah"
      },
      "outputs": [],
      "source": [
        "!pip install gym\n",
        "!pip install Box2D\n",
        "!pip install tetris-gymnasium\n",
        "import gymnasium as gym\n",
        "from tetris_gymnasium.envs import Tetris\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "from gym.wrappers import RecordVideo\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self,in_states,h1_nodes,out_actions):\n",
        "        super().__init__()\n",
        "        self.fc1=nn.Linear(in_states,h1_nodes)\n",
        "        self.out=nn.Linear(h1_nodes,out_actions)\n",
        "    def forward(self,x):\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=self.out(x)\n",
        "        return x\n",
        "class ReplayMemory():\n",
        "    def __init__(self,maxlen):\n",
        "        self.memory=deque([],maxlen=maxlen)\n",
        "    def append(self,transition):\n",
        "        self.memory.append(transition)\n",
        "    def sample(self,sample_size):\n",
        "        return random.sample(self.memory,sample_size)\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "class TetrisDQL():\n",
        "    learning_rate_a=0.001\n",
        "    discount_factor_g=0.90\n",
        "    network_sync_rate=10\n",
        "    replay_memory_size=1000\n",
        "    mini_batch_size=32\n",
        "    loss_fn=nn.MSELoss()\n",
        "    optimizer=None\n",
        "    ACTIONS=['L','R','D','RC','RAC','HARD_DROP','SWAP','NO_Operation']\n",
        "\n",
        "    def preprocess_state(self,state):\n",
        "        state_vector=[]\n",
        "        for key in ['active_tetromino_mask','board','holder','queue']:\n",
        "            state_vector.append(state[key].flatten())\n",
        "        return np.concatenate(state_vector)\n",
        "\n",
        "    def train(self,episodes,render=False):\n",
        "        env=gym.make(\"tetris_gymnasium/Tetris\")\n",
        "\n",
        "\n",
        "        print(f\"Observation Space: {env.observation_space}\")\n",
        "\n",
        "\n",
        "        num_states=sum([space.shape[0] * space.shape[1] for space in env.observation_space.values()])\n",
        "\n",
        "        num_actions=env.action_space.n\n",
        "        epsilon=1\n",
        "        epsilon_min=0.001\n",
        "        epsilon_decay=0.99\n",
        "        memory=ReplayMemory(self.replay_memory_size)\n",
        "        policy_dqn=DQN(in_states=num_states,h1_nodes=64,out_actions=num_actions)\n",
        "        target_dqn=DQN(in_states=num_states,h1_nodes=64,out_actions=num_actions)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "        self.optimizer=torch.optim.Adam(policy_dqn.parameters(),lr=self.learning_rate_a)\n",
        "        rewards_per_episode=np.zeros(episodes)\n",
        "        epsilon_history=[]\n",
        "        score=0\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state=env.reset()\n",
        "            if isinstance(state,tuple):\n",
        "                state=state[0]\n",
        "            state=self.preprocess_state(state)\n",
        "            terminated=False\n",
        "            episode_reward=0\n",
        "            print(f'Episode {i+1}/{episodes},Epsilon: {epsilon:.4f}')\n",
        "            while not terminated:\n",
        "                if random.random() < epsilon:\n",
        "                    action=env.action_space.sample()\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                        action=policy_dqn(torch.FloatTensor(state).unsqueeze(0)).argmax().item()\n",
        "\n",
        "\n",
        "                new_state,reward,terminated,truncated,info=env.step(action)\n",
        "\n",
        "                if isinstance(new_state,tuple):\n",
        "                    new_state=new_state[0]\n",
        "                new_state=self.preprocess_state(new_state)\n",
        "                memory.append((state,action,new_state,reward,terminated))\n",
        "                state=new_state\n",
        "                episode_reward += reward\n",
        "                score += 1\n",
        "\n",
        "            print(f'Episode {i+1} Reward: {episode_reward}')\n",
        "            rewards_per_episode[i]=episode_reward\n",
        "            if len(memory) > self.mini_batch_size:\n",
        "                mini_batch=memory.sample(self.mini_batch_size)\n",
        "                self.optimize(mini_batch,policy_dqn,target_dqn)\n",
        "                epsilon=max(epsilon * epsilon_decay,epsilon_min)\n",
        "                epsilon_history.append(epsilon)\n",
        "                if score > self.network_sync_rate:\n",
        "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                    score=0\n",
        "\n",
        "        env.close()\n",
        "        torch.save(policy_dqn.state_dict(),'tetris_dqn.pt')\n",
        "        plt.subplot(121)\n",
        "        plt.plot(rewards_per_episode,label='Episode Reward')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.title('Reward vs Episode')\n",
        "        plt.legend()\n",
        "        plt.subplot(122)\n",
        "        plt.plot(epsilon_history,label='Epsilon')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Epsilon')\n",
        "        plt.title('Epsilon vs Episode')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def optimize(self,mini_batch,policy_dqn,target_dqn):\n",
        "        num_states=policy_dqn.fc1.in_features\n",
        "        num_actions=policy_dqn.out.out_features\n",
        "        current_q_list=[]\n",
        "        target_q_list=[]\n",
        "        for state,action,new_state,reward,terminated in mini_batch:\n",
        "            if terminated:\n",
        "                target=torch.FloatTensor([reward])\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    target=torch.FloatTensor(\n",
        "                        [reward + self.discount_factor_g * target_dqn(torch.FloatTensor(new_state).unsqueeze(0)).max().item()]\n",
        "                    )\n",
        "            current_q=policy_dqn(torch.FloatTensor(state).unsqueeze(0))\n",
        "            target_q=current_q.clone()\n",
        "            target_q[0][action]=target\n",
        "            current_q_list.append(current_q)\n",
        "            target_q_list.append(target_q)\n",
        "        loss=self.loss_fn(torch.cat(current_q_list),torch.cat(target_q_list))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def test(self,episodes):\n",
        "      env=gym.make(\"tetris_gymnasium/Tetris\",render_mode='rgb_array')\n",
        "      env=RecordVideo(env,video_folder=\"videos\",episode_trigger=lambda e: True,name_prefix=\"test\")\n",
        "      num_states=sum([space.shape[0] * space.shape[1] for space in env.observation_space.values()])\n",
        "      num_actions=env.action_space.n\n",
        "      policy_dqn=DQN(in_states=num_states,h1_nodes=64,out_actions=num_actions)\n",
        "      policy_dqn.load_state_dict(torch.load(\"tetris_dqn.pt\"))\n",
        "      policy_dqn.eval()\n",
        "      for i in range(episodes):\n",
        "          state=env.reset()\n",
        "          if isinstance(state,tuple):\n",
        "              state=state[0]\n",
        "          state=self.preprocess_state(state)\n",
        "          terminated=False\n",
        "          episode_reward=0\n",
        "          print(f'Test Episode {i+1}/{episodes}')\n",
        "          while not terminated:\n",
        "              with torch.no_grad():\n",
        "                  action=policy_dqn(torch.FloatTensor(state).unsqueeze(0)).argmax().item()\n",
        "              new_state,reward,terminated,info=env.step(action)\n",
        "              if isinstance(new_state,tuple):\n",
        "                  new_state=new_state[0]\n",
        "              state=self.preprocess_state(new_state)\n",
        "              episode_reward += reward\n",
        "          print(f'Test Episode {i+1} Reward: {episode_reward}')\n",
        "\n",
        "      env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tetris=TetrisDQL()\n",
        "    tetris.train(20000)\n",
        "    tetris.test(10)\n"
      ],
      "metadata": {
        "id": "KNAq_0J4vgvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3mwOco8y0Bou"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}